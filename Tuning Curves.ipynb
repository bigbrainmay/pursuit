{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages and functions\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pursuit_functions as pursuit\n",
    "    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data set\n",
    "\n",
    "all_pursuit_tasks = pd.read_parquet(\"ca1_ca3_rsc_pursuit_data.parquet\", engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Normalize points and find circle boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all coordinate values below 99th percentile and normalize points for all regions \n",
    "\n",
    "normalized_sessions = pursuit.tuning.normalize_points(all_pursuit_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_sessions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the mean center and overall radius of the arena for all normalized data points\n",
    "#you can specify the percentile value to be considered for the overall radius; default is 95th percentile\n",
    "#calculates the individual center point for each session\n",
    "\n",
    "circle_boundaries, radius = pursuit.tuning.fit_circle_bounds(normalized_sessions)\n",
    "print(radius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "circle_boundaries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find circumference points for plotting using the center coordinates and overall radius\n",
    "all_circ_points = pursuit.tuning.circumference(circle_boundaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "# Plot the laser coordinates and boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot normalized concatenated laser and rat paths with center point and boundary\n",
    "#the function takes the normalized_sessions, circle_boundaries, and all_circ_points dataframes\n",
    "\n",
    "pursuit.tuning.plot_arena_bounds(normalized_sessions, circle_boundaries, all_circ_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# Clean data and pull spike data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain region-specific sessions\n",
    "RSC_sessions = all_pursuit_tasks[all_pursuit_tasks[\"region\"] == \"RSC\"]\n",
    "CA1_sessions = all_pursuit_tasks[all_pursuit_tasks[\"region\"] == \"CA1\"]\n",
    "CA3_sessions = all_pursuit_tasks[all_pursuit_tasks[\"region\"] == \"CA3\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain trial block-specific sessions\n",
    "\n",
    "RSC_pursuit = RSC_sessions[RSC_sessions[\"trial_block\"] == \"pursuit\"]\n",
    "CA1_pursuit = CA1_sessions[CA1_sessions[\"trial_block\"] == \"pursuit\"]\n",
    "CA3_pursuit = CA3_sessions[CA3_sessions[\"trial_block\"] == \"pursuit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop NA values for RSC, CA1, and CA3 sessions\n",
    "\n",
    "RSC_cleaned = pursuit.tuning.drop_NA_vals(RSC_pursuit)\n",
    "CA1_cleaned = pursuit.tuning.drop_NA_vals(CA1_pursuit)\n",
    "CA3_cleaned = pursuit.tuning.drop_NA_vals(CA3_pursuit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there's some data compression that collapses the 120hz recording time points as the recordings get longer. \n",
    "# first we collapse the time points to whole seconds\n",
    "# we need to normalize the time so that the first observation starts from 0 seconds\n",
    "# we then calculate the normalized minute each observation belongs to\n",
    "\n",
    "def normalize_time(dataframe):\n",
    "    df = dataframe.copy()\n",
    "    df[\"time\"] = df[\"time\"].astype(float)\n",
    "    df[\"relative_time\"] = df.groupby(\"sessFile\")[\"time\"].transform(lambda x: x - x.min())\n",
    "    df[\"norm_sec\"] = df[\"relative_time\"].astype(int)\n",
    "    df[\"norm_min\"] = df[\"norm_sec\"] // 60\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "RSC_clean_time = normalize_time(RSC_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "RSC_clean_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to split each session into epochs, we separate them by first/second half of the recording or odd/even minutes\n",
    "# for epoch_half, True:1, False:2 will result in the first half labeled as 1 (<= cutoff) and the second half labeled as 2\n",
    "# for epoch_odd_even, we take the minutes divided by 2 and find the remainder. If the remainder is 1 (odd), it will be labeled as 1 and if the remainder is 0 (even), it will be labeled as 2\n",
    "\n",
    "def assign_epochs(dataframe):\n",
    "    df = dataframe.copy()\n",
    "\n",
    "    # separate epochs by half\n",
    "\n",
    "    def label_half(group):\n",
    "        mins = group[\"norm_min\"].unique()\n",
    "        mins.sort()\n",
    "        cutoff = mins[len(mins) // 2]\n",
    "        return group[\"norm_min\"] <= cutoff\n",
    "    \n",
    "    df[\"epoch_half\"] = (\n",
    "        df.groupby(\"sessFile\", group_keys=False)\n",
    "        .apply(label_half, include_groups=False)\n",
    "        .map({True: 1, False: 2})\n",
    "    )\n",
    "\n",
    "    # separate epochs by odd/even minutes\n",
    "\n",
    "    df[\"epoch_odd_even\"] = df[\"norm_min\"] % 2\n",
    "    df[\"epoch_odd_even\"] = df[\"epoch_odd_even\"].map({1: 1, 0: 2})\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch_half, True:1, False:2 will result in the first half labeled as 1 (<= cutoff) and the second half labeled as 2\n",
    "# for epoch_odd_even, we take the minutes divided by 2 and find the remainder. If the remainder is 1 (odd), it will be labeled as 1 and if the remainder is 0 (even), it will be labeled as 2\n",
    "\n",
    "RSC_clean_time_epochs = assign_epochs(RSC_clean_time)\n",
    "\n",
    "RSC_clean_time_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "RSC_clean_time_epochs[RSC_clean_time_epochs[\"sessFile\"] == \"KB10_02_pursuitRoot.mat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a new df with only sessFile, laser, epoch, and spike data\n",
    "def epoch_laser_spks(dataframe, laser_x=\"laserPos_1\", laser_y=\"laserPos_2\"):\n",
    "\n",
    "    epoch_laser_spks_data = []\n",
    "\n",
    "    spk_columns = [col for col in dataframe.columns if \"spkTable\" in col]\n",
    "    \n",
    "    for sessFile in dataframe[\"sessFile\"].unique():\n",
    "\n",
    "            session = dataframe[dataframe[\"sessFile\"] == sessFile].copy()\n",
    "\n",
    "            laser_x_vals = session[laser_x].astype(\"float64\")\n",
    "            laser_y_vals = session[laser_y].astype(\"float64\")\n",
    "    \n",
    "            #identify 99th percentile x, y boundaries\n",
    "            x_low, x_high = np.percentile(laser_x_vals, [0, 99])\n",
    "            y_low, y_high = np.percentile(laser_y_vals, [0, 99])\n",
    "\n",
    "            #filter the data so we only get the data under the 99th percentile\n",
    "            filter = (\n",
    "                (laser_x_vals >= x_low) & (laser_x_vals <= x_high) & \n",
    "                (laser_y_vals >= y_low) & (laser_y_vals <= y_high)\n",
    "            )\n",
    "\n",
    "            filtered_session = session[filter].copy()\n",
    "\n",
    "            #normalize the points to the origin\n",
    "            x_normalized = filtered_session[laser_x].astype(\"float64\") - float(x_low)\n",
    "            y_normalized = filtered_session[laser_y].astype(\"float64\") - float(y_low)\n",
    "\n",
    "            #grab epoch data \n",
    "            epoch_half = filtered_session[\"epoch_half\"].values\n",
    "            epoch_odd_even = filtered_session[\"epoch_odd_even\"].values\n",
    "\n",
    "            #make a dataframe containing normalized data\n",
    "            normalized_df = pd.DataFrame({\n",
    "                \"sessFile\": sessFile,\n",
    "                \"laser_x_normalized\": x_normalized.values,\n",
    "                \"laser_y_normalized\": y_normalized.values,\n",
    "                \"epoch_half\": epoch_half,\n",
    "                \"epoch_odd_even\": epoch_odd_even\n",
    "            })\n",
    "\n",
    "            #grab spike data using the normalized data mask\n",
    "            spk_df = filtered_session[spk_columns].reset_index(drop=True)\n",
    "\n",
    "            #make a combined dataframe\n",
    "            combined_df = pd.concat([normalized_df.reset_index(drop=True), spk_df], axis=1)\n",
    "\n",
    "            #append dataframe to the list\n",
    "            epoch_laser_spks_data.append(combined_df)\n",
    "\n",
    "        #make a giant dataframe by concatenating all the dataframes in the list        \n",
    "    epoch_laser_spks_df = pd.concat(epoch_laser_spks_data, ignore_index=True)\n",
    "\n",
    "    return epoch_laser_spks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "RSC_epoch_laser_spks = epoch_laser_spks(RSC_clean_time_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "RSC_epoch_laser_spks[RSC_epoch_laser_spks[\"sessFile\"] == \"KB10_02_pursuitRoot.mat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for pulling epochs from each session along with associated sessFile, laser, and spike data\n",
    "\n",
    "def pull_epochs(dataframe, \n",
    "                spk_prefix=\"spkTable\"):\n",
    "\n",
    "    epoch_first_half = []\n",
    "    epoch_second_half = []\n",
    "    epoch_odd_min = []\n",
    "    epoch_even_min = []\n",
    "\n",
    "    for sessFile in dataframe[\"sessFile\"].unique():\n",
    "        \n",
    "        session = dataframe[dataframe[\"sessFile\"] == sessFile].copy()\n",
    "\n",
    "        # mapping first half and second epochs for each session \n",
    "        first_half = session[session[\"epoch_half\"] == 1]\n",
    "        second_half = session[session[\"epoch_half\"] ==2]\n",
    "        # mapping odd and even epochs for each session\n",
    "        odd_min = session[session[\"epoch_odd_even\"] == 1]\n",
    "        even_min = session[session[\"epoch_odd_even\"] == 2]\n",
    "\n",
    "        spk_cols = [col for col in session.columns if spk_prefix in col and not session[col].isna().all()]\n",
    "\n",
    "        #function for grabbing sessFile, laser x, laser y, and spk columns for each epoch\n",
    "        def build_epoch_df(epoch):\n",
    "            return pd.concat([\n",
    "                epoch[[\"sessFile\", \"laser_x_normalized\", \"laser_y_normalized\"]].reset_index(drop=True),\n",
    "                epoch[spk_cols].reset_index(drop=True)\n",
    "            ], axis=1)\n",
    "\n",
    "        epoch_first_half.append(build_epoch_df(first_half))\n",
    "        epoch_second_half.append(build_epoch_df(second_half))\n",
    "        epoch_odd_min.append(build_epoch_df(odd_min))\n",
    "        epoch_even_min.append(build_epoch_df(even_min))\n",
    "\n",
    "    return (\n",
    "        pd.concat(epoch_first_half, ignore_index=True),\n",
    "        pd.concat(epoch_second_half, ignore_index=True),\n",
    "        pd.concat(epoch_odd_min, ignore_index=True),\n",
    "        pd.concat(epoch_even_min, ignore_index=True),\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "RSC_epoch_first_half, RSC_epoch_second_half, RSC_epoch_odd_min, RSC_epoch_even_min = pull_epochs(RSC_epoch_laser_spks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "RSC_epoch_first_half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning curves for first half and second half epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning curves for odd and even min epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "shift spikes circularly for bootstrapping x1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_time_entries(dataframe):\n",
    "    all_sessions = []\n",
    "\n",
    "    for sessFile in dataframe[\"sessFile\"].unique():\n",
    "        session = dataframe[dataframe[\"sessFile\"] == sessFile]\n",
    "        session_times = session[\"time\"]\n",
    "\n",
    "        times_unique = session_times.astype(\"float64\").nunique()\n",
    "        times_count = session_times.astype(\"float64\").value_counts().to_dict()\n",
    "\n",
    "        all_sessions.append({\n",
    "            \"sessFile\": sessFile,\n",
    "            \"times_unique\": times_unique,\n",
    "            \"unique_times_count\": times_count\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(all_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "RSC_sessions_times = unique_time_entries(RSC_sessions)\n",
    "RSC_sessions_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_time_counts(session_row):\n",
    "    times= session_row[\"unique_times_count\"]\n",
    "    sorted_times = sorted(times.items())\n",
    "    return pd.DataFrame(sorted_times, columns=[\"time\", \"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "lp03_25 = RSC_sessions_times[RSC_sessions_times[\"sessFile\"] == \"LP03_25_pursuitRoot.mat\"]\n",
    "\n",
    "lp03_25_row = lp03_25.iloc[0]\n",
    "\n",
    "lp03_25_inspection = inspect_time_counts(lp03_25_row)\n",
    "\n",
    "\n",
    "lp03_25_inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "RSC_cleaned.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize only laser points and make a dataframe containing spike data using the normalized data mask\n",
    "#function takes the cleaned df\n",
    "RSC_laser_spks = pursuit.tuning.norm_laser_get_spks(RSC_cleaned)\n",
    "CA1_laser_spks = pursuit.tuning.norm_laser_get_spks(CA1_cleaned)\n",
    "CA3_laser_spks = pursuit.tuning.norm_laser_get_spks(CA3_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "CA1_laser_spks.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "# Find distance of laser points to boundary and bin data by distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find distance of normalized laser points to circle boundary by each session\n",
    "#function takes the normalized laser/spikes and circle boundaries dataframes\n",
    "\n",
    "RSC_laser_spks_bounds = pursuit.tuning.dist_to_bounds(RSC_laser_spks, circle_boundaries)\n",
    "CA1_laser_spks_bounds = pursuit.tuning.dist_to_bounds(CA1_laser_spks, circle_boundaries)\n",
    "CA3_laser_spks_bounds = pursuit.tuning.dist_to_bounds(CA3_laser_spks, circle_boundaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "CA3_laser_spks_bounds.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bin the data!\n",
    "RSC_laser_spikes_binned = pursuit.tuning.bin_spikes_laser(RSC_laser_spks_bounds)\n",
    "CA1_laser_spikes_binned = pursuit.tuning.bin_spikes_laser(CA1_laser_spks_bounds)\n",
    "CA3_laser_spikes_binned = pursuit.tuning.bin_spikes_laser(CA3_laser_spks_bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "# Normalize spike counts, smooth data, peak sort neurons, and plot tuning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate raw tuning curves\n",
    "RSC_tuning = pursuit.tuning.calculate_tuning(RSC_laser_spikes_binned)\n",
    "CA1_tuning = pursuit.tuning.calculate_tuning(CA1_laser_spikes_binned)\n",
    "CA3_tuning = pursuit.tuning.calculate_tuning(CA3_laser_spikes_binned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot all neuron tuning curves for a sanity check\n",
    "\n",
    "pursuit.tuning.plot_tuning_curves(CA3_tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# z-score and normalize data\n",
    "RSC_z_scored = pursuit.tuning.z_score_norm(RSC_tuning)\n",
    "CA1_z_scored = pursuit.tuning.z_score_norm(CA1_tuning)\n",
    "CA3_z_scored = pursuit.tuning.z_score_norm(CA3_tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply smoothing and pivot data\n",
    "RSC_smoothed = pursuit.tuning.pivot_smooth(RSC_z_scored)\n",
    "CA1_smoothed = pursuit.tuning.pivot_smooth(CA1_z_scored)\n",
    "CA3_smoothed = pursuit.tuning.pivot_smooth(CA3_z_scored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# peak sort the data\n",
    "RSC_smoothed_sorted = pursuit.tuning.peak_sort(RSC_smoothed)\n",
    "CA1_smoothed_sorted = pursuit.tuning.peak_sort(CA1_smoothed)\n",
    "CA3_smoothed_sorted = pursuit.tuning.peak_sort(CA3_smoothed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot heatmaps\n",
    "pursuit.tuning.heatmap(RSC_smoothed_sorted)\n",
    "pursuit.tuning.heatmap(CA1_smoothed_sorted)\n",
    "pursuit.tuning.heatmap(CA3_smoothed_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "RSC_count = pursuit.tuning.count_neurons(RSC_cleaned)\n",
    "CA1_count = pursuit.tuning.count_neurons(CA1_cleaned)\n",
    "CA3_count = pursuit.tuning.count_neurons(CA3_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "RSC_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "CA1_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "CA3_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pursuit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
