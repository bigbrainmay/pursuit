{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages and functions\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pursuit_functions as pursuit\n",
    "    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data set\n",
    "\n",
    "all_pursuit_tasks = pd.read_parquet(\"ca1_ca3_rsc_pursuit_data.parquet\", engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop NA values for RSC, CA1, and CA3 sessions\n",
    "RSC_sessions = all_pursuit_tasks[all_pursuit_tasks[\"region\"] == \"RSC\"]\n",
    "CA1_sessions = all_pursuit_tasks[all_pursuit_tasks[\"region\"] == \"CA1\"]\n",
    "CA3_sessions = all_pursuit_tasks[all_pursuit_tasks[\"region\"] == \"CA3\"]\n",
    "\n",
    "def drop_NA_vals(dataframe):\n",
    "    cleaned_data = dataframe.dropna(subset=['ratPos_1', 'ratPos_2', 'laserPos_1', 'laserPos_2'])\n",
    "    return cleaned_data\n",
    "\n",
    "RSC_sessions_cleaned = drop_NA_vals(RSC_sessions)\n",
    "CA1_sessions_cleaned = drop_NA_vals(CA1_sessions)\n",
    "CA3_sessions_cleaned = drop_NA_vals(CA3_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all coordinate values below 99th percentile and normalize points \n",
    "\n",
    "def normalize_points(dataframe, rat_x, rat_y, laser_x, laser_y):\n",
    "\n",
    "    normalized_data = []\n",
    "    \n",
    "    for sessFile in dataframe[\"sessFile\"].unique():\n",
    "        session = dataframe[dataframe[\"sessFile\"] == sessFile].dropna(subset=[rat_x, rat_y, laser_x, laser_y]).copy()\n",
    "\n",
    "        region = dataframe['region'][0]\n",
    "\n",
    "        rat_x_vals = session[rat_x].values.astype(\"float64\")\n",
    "        rat_y_vals = session[rat_y].values.astype(\"float64\")\n",
    "        laser_x_vals = session[laser_x].values.astype(\"float64\")\n",
    "        laser_y_vals = session[laser_y].values.astype(\"float64\")\n",
    "\n",
    "        x = np.concatenate((rat_x_vals, laser_x_vals))\n",
    "        y = np.concatenate((rat_y_vals, laser_y_vals))\n",
    "\n",
    "        #identify 99th percentile x, y boundaries\n",
    "        x_low, x_high = np.percentile(x, [1, 99])\n",
    "        y_low, y_high = np.percentile(y, [1, 99])\n",
    "\n",
    "        #filter the data so we only get the data under the 99th percentile\n",
    "        filter = (x >= x_low) & (x <= x_high) & (y >= y_low) & (y <= y_high)\n",
    "\n",
    "        x_filtered = x[filter]\n",
    "        y_filtered = y[filter]\n",
    "\n",
    "        #normalize the points to the origin\n",
    "        x_normalized = x_filtered - x_low\n",
    "        y_normalized = y_filtered - y_low\n",
    "\n",
    "        for xn, yn in zip(x_normalized, y_normalized):\n",
    "            normalized_data.append({\n",
    "                \"sessFile\": sessFile,\n",
    "                \"region\": region,\n",
    "                \"x_normalized\": xn,\n",
    "                \"y_normalized\": yn,\n",
    "                \"x_low\": x_low,\n",
    "                \"x_high\": x_high, \n",
    "                \"y_low\": y_low, \n",
    "                \"y_high\": y_high\n",
    "            })\n",
    "            \n",
    "    normalized_df = pd.DataFrame(normalized_data)\n",
    "    return normalized_df\n",
    "\n",
    "normalized_sessions = normalize_points(all_pursuit_tasks, \"ratPos_1\", \"ratPos_2\", \"laserPos_1\", \"laserPos_2\")\n",
    "#normalized_sessions.head()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#boundary approach: find center of arena and radius to find boundaries for all normalized data points\n",
    "def circle_fit_bounds(dataframe, x_norm, y_norm, radius_percentile=95):\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    all_x_vals = dataframe[x_norm].values.astype(\"float64\")\n",
    "    all_y_vals = dataframe[y_norm].values.astype(\"float64\")\n",
    "\n",
    "    overall_center_x = np.mean(all_x_vals)\n",
    "    overall_center_y = np.mean(all_y_vals)\n",
    "\n",
    "    overall_distances = np.sqrt((all_x_vals - overall_center_x)**2 + (all_y_vals - overall_center_y)**2)\n",
    "\n",
    "    overall_radius = np.percentile(overall_distances, radius_percentile)\n",
    "\n",
    "    for sessFile in dataframe[\"sessFile\"].unique():\n",
    "        session = dataframe[dataframe[\"sessFile\"] == sessFile]\n",
    "        region = dataframe['region'][0]\n",
    "\n",
    "        session_x_vals = session[x_norm].values.astype(\"float64\")\n",
    "        session_y_vals = session[y_norm].values.astype(\"float64\")\n",
    "\n",
    "        x_min = np.min(session_x_vals)\n",
    "        x_max = np.max(session_x_vals)\n",
    "        y_min = np.min(session_y_vals)\n",
    "        y_max = np.max(session_y_vals)\n",
    "\n",
    "        center_x = (x_min + x_max) / 2\n",
    "        center_y = (y_min + y_max) / 2\n",
    "\n",
    "        results.append({\n",
    "            \"sessFile\": sessFile,\n",
    "            \"region\": region,\n",
    "            \"center_x\": center_x,\n",
    "            \"center_y\": center_y,\n",
    "            \"x_min\": x_min,\n",
    "            \"x_max\": x_max,\n",
    "            \"y_min\": y_min,\n",
    "            \"y_max\": y_max,\n",
    "            \"radius\": overall_radius\n",
    "        })\n",
    "\n",
    "    \n",
    "    center_radius_vals = pd.DataFrame(results)\n",
    "\n",
    "    return center_radius_vals, overall_radius\n",
    "\n",
    "   \n",
    "normalized_center_radius, radius = circle_fit_bounds(normalized_sessions, \"x_normalized\", \"y_normalized\", radius_percentile=95)\n",
    "\n",
    "print(radius)\n",
    "#normalized_center_radius.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find circle boundary points\n",
    "def circ_bounds(dataframe, center_x, center_y, radius):\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    for sessFile in dataframe[\"sessFile\"].unique():\n",
    "        session = dataframe[dataframe[\"sessFile\"] == sessFile]\n",
    "\n",
    "        x_val = session[center_x].values.astype(\"float64\")\n",
    "        y_val = session[center_y].values.astype(\"float64\")\n",
    "        r = session[radius].values.astype(\"float64\")\n",
    "\n",
    "        num_points = 360\n",
    "\n",
    "        angles = np.linspace(0, 2 * np.pi, num_points, endpoint=False)\n",
    "\n",
    "        x_points = x_val + r * np.cos(angles)\n",
    "        y_points = y_val + r * np.sin(angles)\n",
    "\n",
    "        results.append({\n",
    "            \"sessFile\": sessFile,\n",
    "            \"x_bounds\": x_points,\n",
    "            \"y_bounds\": y_points,\n",
    "            \"radius\": r\n",
    "        })\n",
    "    \n",
    "    session_bound_radius_vals = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "    return session_bound_radius_vals\n",
    "\n",
    "#circumference, x_circ, y_circ = circ_bounds(mean_center_x, mean_center_y, mean_radius)\n",
    "all_circ_bounds = circ_bounds(normalized_center_radius, \"center_x\", \"center_y\", \"radius\")\n",
    "\n",
    "\n",
    "all_circ_bounds.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot normalized concatenated laser and rat paths with center point and boundary\n",
    "for sessFile in normalized_sessions[\"sessFile\"].unique():\n",
    "    subset_sessFile = normalized_sessions[normalized_sessions[\"sessFile\"] == sessFile]\n",
    "    plt.plot(subset_sessFile[\"x_normalized\"], subset_sessFile[\"y_normalized\"], color='purple', label=f'Session {sessFile}')\n",
    "    \n",
    "    subset_center = normalized_center_radius[normalized_center_radius[\"sessFile\"] == sessFile]\n",
    "    plt.scatter(subset_center[\"center_x\"], subset_center[\"center_y\"], color='black', zorder=5)\n",
    "\n",
    "    subset_bounds = all_circ_bounds[all_circ_bounds[\"sessFile\"] == sessFile]\n",
    "    plt.plot(subset_bounds[\"x_bounds\"].iloc[0], subset_bounds[\"y_bounds\"].iloc[0], 'b-', zorder=10, label='Circle Boundary')\n",
    "   \n",
    "    plt.axis('equal')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize laser points and make a dataframe containing spike data using the normalized data mask\n",
    "def normalize_laser_points(dataframe, laser_x, laser_y):\n",
    "\n",
    "    normalized_laser_data = []\n",
    "\n",
    "    spk_columns = [col for col in dataframe.columns if \"spkTable\" in col]\n",
    "    \n",
    "    for sessFile in dataframe[\"sessFile\"].unique():\n",
    "\n",
    "        session = dataframe[dataframe[\"sessFile\"] == sessFile]\n",
    "        laser_x_vals = session[laser_x].values.astype(\"float64\")\n",
    "        laser_y_vals = session[laser_y].values.astype(\"float64\")\n",
    "   \n",
    "        #identify 99th percentile x, y boundaries\n",
    "        x_low, x_high = np.percentile(laser_x_vals, [0, 99])\n",
    "        y_low, y_high = np.percentile(laser_y_vals, [0, 99])\n",
    "\n",
    "        #filter the data so we only get the data under the 99th percentile\n",
    "        filter = (laser_x_vals >= x_low) & (laser_x_vals <= x_high) & (laser_y_vals >= y_low) & (laser_y_vals <= y_high)\n",
    "        x_filtered = laser_x_vals[filter]\n",
    "        y_filtered = laser_y_vals[filter]\n",
    "\n",
    "        #normalize the points to the origin\n",
    "        x_normalized = x_filtered - x_low\n",
    "        y_normalized = y_filtered - y_low\n",
    "\n",
    "        #make a dataframe containing normalized data\n",
    "        normalized_df = pd.DataFrame({\n",
    "            \"sessFile\": sessFile,\n",
    "            \"laser_x_normalized\": x_normalized,\n",
    "            \"laser_y_normalized\": y_normalized\n",
    "        })\n",
    "\n",
    "        #make a dataframe containing spike data using the normalized data mask\n",
    "        spk_df = session.loc[filter, spk_columns].reset_index(drop=True)\n",
    "\n",
    "        #make a combined dataframe\n",
    "        combined_df = pd.concat([normalized_df.reset_index(drop=True), spk_df], axis=1)\n",
    "\n",
    "        #append dataframe to the list\n",
    "        normalized_laser_data.append(combined_df)\n",
    "\n",
    "    #make a giant dataframe by concatenating all the dataframes in the list        \n",
    "    laser_normalized_df = pd.concat(normalized_laser_data, ignore_index=True)\n",
    "\n",
    "    return laser_normalized_df\n",
    "\n",
    "\n",
    "RSC_sessions_laser_normalized = normalize_laser_points(RSC_sessions_cleaned, \"laserPos_1\", \"laserPos_2\")\n",
    "CA1_sessions_laser_normalized = normalize_laser_points(CA1_sessions_cleaned, \"laserPos_1\", \"laserPos_2\")\n",
    "CA3_sessions_laser_normalized = normalize_laser_points(CA3_sessions_cleaned, \"laserPos_1\", \"laserPos_2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find distance of normalized laser points to boundary\n",
    "\n",
    "def laser_bound_dist(laser_df, center_df, laser_x, laser_y, center_x, center_y, radius):\n",
    "\n",
    "    session_df = [] \n",
    "\n",
    "    for sessFile in laser_df[\"sessFile\"].unique():\n",
    "        session = laser_df[laser_df[\"sessFile\"] == sessFile].copy()\n",
    "\n",
    "        laser_x_val = session[laser_x].values.astype(\"float64\")\n",
    "        laser_y_val = session[laser_y].values.astype(\"float64\")\n",
    "\n",
    "        subset_center = center_df[center_df[\"sessFile\"] == sessFile]\n",
    "\n",
    "        center_x_val = subset_center[center_x].iloc[0]\n",
    "        center_y_val = subset_center[center_y].iloc[0]\n",
    "        r = subset_center[radius].iloc[0]\n",
    "\n",
    "        session[\"center_dist\"] = np.sqrt((laser_x_val - center_x_val)**2 + (laser_y_val - center_y_val)**2)\n",
    "\n",
    "        session[\"bound_dist\"] = np.abs(session[\"center_dist\"] - r)\n",
    "\n",
    "        session_df.append(session)\n",
    "\n",
    "    combined_df = pd.concat(session_df, ignore_index=True)\n",
    "    return combined_df\n",
    "\n",
    "RSC_sessions_laser_bounds_spks = laser_bound_dist(RSC_sessions_laser_normalized, normalized_center_radius, \"laser_x_normalized\", \"laser_y_normalized\", \"center_x\", \"center_y\", \"radius\")\n",
    "CA1_sessions_laser_bounds_spks = laser_bound_dist(CA1_sessions_laser_normalized, normalized_center_radius, \"laser_x_normalized\", \"laser_y_normalized\", \"center_x\", \"center_y\", \"radius\")\n",
    "CA3_sessions_laser_bounds_spks = laser_bound_dist(CA3_sessions_laser_normalized, normalized_center_radius, \"laser_x_normalized\", \"laser_y_normalized\", \"center_x\", \"center_y\", \"radius\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for calculating bin edges from overall min and max bound_dist values\n",
    "def find_overall_bin_edges(dataframe, column, num_bins):\n",
    "    overall_min = dataframe[column].min()\n",
    "    overall_max = dataframe[column].max()\n",
    "\n",
    "    bin_edges = np.linspace(overall_min, overall_max, num_bins +1)\n",
    "    return bin_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#put raw spike counts into bins calculated from the overall min and max bound_dist values\n",
    "def bin_spike_data(dataframe, spk_prefix=\"spkTable\", num_bins=20, bin_edges=None):\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    if bin_edges is None:\n",
    "        bin_edges = find_overall_bin_edges(dataframe, \"bound_dist\", num_bins)\n",
    "\n",
    "    for sessFile in dataframe[\"sessFile\"].unique():\n",
    "\n",
    "        session = dataframe[dataframe[\"sessFile\"] == sessFile].copy()\n",
    "\n",
    "        session[\"bound_bin\"] = pd.cut(session[\"bound_dist\"], bins=bin_edges, include_lowest=True)\n",
    "\n",
    "        spk_cols = [col for col in session.columns if spk_prefix in col and not session[col].isna().all()]\n",
    "\n",
    "        for spk in spk_cols:\n",
    "            spk_by_bin = session.groupby(\"bound_bin\")[spk].sum()\n",
    "\n",
    "            bin_midpoints = pd.IntervalIndex.from_breaks(bin_edges).to_series().apply(\n",
    "                lambda interval: round((interval.left + interval.right) / 2, 2)\n",
    "                )\n",
    "\n",
    "            for bin_mid, spk_count in zip(bin_midpoints, spk_by_bin):\n",
    "                rows.append({\n",
    "                    \"sessFile\": sessFile,\n",
    "                    \"neuron\": spk,\n",
    "                    \"spike_count\": spk_count,\n",
    "                    \"bin_midpoint\": bin_mid        \n",
    "                })\n",
    "\n",
    "    binned_spks_df = pd.DataFrame(rows)\n",
    "    return binned_spks_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#put laser coordinates into bins calculated from the overall min and max bound_dist values\n",
    "def bin_laser_data(dataframe, num_bins=20, bin_edges=None):\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    if bin_edges is None:\n",
    "        bin_edges = find_overall_bin_edges(dataframe, \"bound_dist\", num_bins)\n",
    "\n",
    "    for sessFile in dataframe[\"sessFile\"].unique():\n",
    "\n",
    "        session = dataframe[dataframe[\"sessFile\"] == sessFile].copy()\n",
    "\n",
    "        session[\"bound_bin\"] = pd.cut(session[\"bound_dist\"], bins=bin_edges, include_lowest=True)\n",
    "\n",
    "        coords_by_bin = session.groupby(\"bound_bin\").size()\n",
    "\n",
    "        for bin_interval, laser_count in coords_by_bin.items():\n",
    "            bin_mid = round((bin_interval.left + bin_interval.right) / 2, 2)\n",
    "            \n",
    "            rows.append({\n",
    "                    \"sessFile\": sessFile,\n",
    "                    \"laser_occupancy\": laser_count,\n",
    "                    \"bin_midpoint\": bin_mid\n",
    "                })\n",
    "\n",
    "    binned_laser_df = pd.DataFrame(rows)\n",
    "    return binned_laser_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bin the data!\n",
    "\n",
    "RSC_laser_binned = bin_laser_data(RSC_sessions_laser_bounds_spks)\n",
    "CA1_laser_binned = bin_laser_data(CA1_sessions_laser_bounds_spks)\n",
    "CA3_laser_binned = bin_laser_data(CA3_sessions_laser_bounds_spks)\n",
    "\n",
    "RSC_spikes_binned = bin_spike_data(RSC_sessions_laser_bounds_spks)\n",
    "CA1_spikes_binned = bin_spike_data(CA1_sessions_laser_bounds_spks)\n",
    "CA3_spikes_binned = bin_spike_data(CA3_sessions_laser_bounds_spks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize spike counts by laser occupancy using bins calculated from the overall min and max bound_dist values \n",
    "\n",
    "def make_tuning_curve(spike_df, laser_df):\n",
    "\n",
    "    merged_df = pd.merge(spike_df, laser_df, on=[\"sessFile\", \"bin_midpoint\"], how=\"left\")\n",
    "\n",
    "    merged_df[\"tuning\"] = merged_df[\"spike_count\"] / merged_df[\"laser_occupancy\"]\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "RSC_binned_tuning = make_tuning_curve(RSC_spikes_binned, RSC_laser_binned)\n",
    "CA1_binned_tuning = make_tuning_curve(CA1_spikes_binned, CA1_laser_binned)\n",
    "CA3_binned_tuning = make_tuning_curve(CA3_spikes_binned, CA3_laser_binned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tuning_curves(dataframe):\n",
    "    \n",
    "    plt.figure(figsize=(12,8))\n",
    "\n",
    "    for sessFile in dataframe[\"sessFile\"].unique():\n",
    "\n",
    "        session = dataframe[dataframe[\"sessFile\"] == sessFile]\n",
    "        \n",
    "        pivoted = (session.pivot(index=\"neuron\", columns=\"bin_midpoint\", values=\"tuning\").fillna(0))\n",
    "\n",
    "        for neuron in pivoted.index:\n",
    "            plt.plot(pivoted.columns, pivoted.loc[neuron], marker='o', linestyle='-', label=f\"{neuron}\")\n",
    "\n",
    "    plt.xlabel(\"Boundary Distance (bin midpoint)\")\n",
    "    plt.ylabel(\"Tuning (spike count / laser occupancy)\")\n",
    "    plt.title(f\"All Neuron Tuning Curves\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tuning_curves(RSC_binned_tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "RSC_binned_tuning.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#z-score binned normalized data\n",
    "\n",
    "def z_score_norm(dataframe):\n",
    "    \n",
    "    sessions = dataframe.copy()\n",
    "        \n",
    "    def norm_z(x):\n",
    "        std = x.std()\n",
    "        mean = x.mean()\n",
    "        z = (x - mean) / std if std > 0 else x*0\n",
    "\n",
    "        z_min, z_max = z.min(), z.max()\n",
    "        if z_max > z_min:\n",
    "            return (z - z_min) / (z_max - z_min)\n",
    "        else:\n",
    "            return z * 0\n",
    "\n",
    "        \n",
    "    sessions[\"z_score\"] = (\n",
    "        sessions.groupby([\"sessFile\", \"neuron\"])[\"tuning\"].transform(norm_z)\n",
    "    )\n",
    "\n",
    "    return sessions\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "RSC_z_scored = z_score_norm(RSC_binned_tuning)\n",
    "CA1_z_scored = z_score_norm(CA1_binned_tuning)\n",
    "CA3_z_scored = z_score_norm(CA3_binned_tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot heatmap for z-scored data: RSC sessions\n",
    "\n",
    "pivoted_binned_zscore_neurons = RSC_z_scored.pivot_table(\n",
    "    index=[\"sessFile\", \"neuron\"], \n",
    "    columns=\"bin_midpoint\", \n",
    "    values=\"z_score\"\n",
    "    ).fillna(0)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "heatmap = sns.heatmap(pivoted_binned_zscore_neurons, cmap=\"viridis\", annot=False, fmt=\".2f\", yticklabels=False)\n",
    "\n",
    "x_labels = heatmap.get_xticklabels()\n",
    "\n",
    "rounded_labels = [f\"{float(label.get_text()):.2f}\" if label.get_text() != \"\" else \"\" for label in x_labels]\n",
    "\n",
    "heatmap.set_xticklabels(rounded_labels, rotation=45, ha=\"right\")\n",
    "\n",
    "\n",
    "plt.title(f\"Z-scored Spike Activity by Boundary Distance: RSC Sessions\")\n",
    "plt.xlabel(\"Boundary Distance (bin midpoint)\")\n",
    "plt.ylabel(\"Neurons\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pivot table and apply gaussian smoothing for plotting\n",
    "\n",
    "def pivot_smooth(dataframe, window_size=3, window_type='gaussian', std=1):\n",
    "\n",
    "    pivoted_df = dataframe.pivot_table(\n",
    "        index=[\"sessFile\", \"neuron\"], \n",
    "        columns=\"bin_midpoint\", \n",
    "        values=\"z_score\"\n",
    "        ).fillna(0)\n",
    "\n",
    "    smoothed_df = pivoted_df.apply(\n",
    "        lambda row: row.rolling(\n",
    "            window=window_size, \n",
    "            win_type=window_type, \n",
    "            center=True).mean(std=std), \n",
    "            axis=1\n",
    "        ).fillna(0)\n",
    "    \n",
    "    return smoothed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot heatmap for z-scored data\n",
    "\n",
    "def heatmap(dataframe):\n",
    "\n",
    "    plt.figure(figsize=(12,8))\n",
    "\n",
    "    heatmap = sns.heatmap(dataframe, cmap=\"viridis\", annot=False, fmt=\".2f\", yticklabels=False)\n",
    "\n",
    "    x_labels = heatmap.get_xticklabels()\n",
    "\n",
    "    rounded_labels = [f\"{float(label.get_text()):.2f}\" if label.get_text() != \"\" else \"\" for label in x_labels]\n",
    "\n",
    "    heatmap.set_xticklabels(rounded_labels, rotation=45, ha=\"right\")\n",
    "\n",
    "\n",
    "    plt.title(f\"Z-scored Spike Activity by Boundary Distance\")\n",
    "    plt.xlabel(\"Boundary Distance (bin midpoint)\")\n",
    "    plt.ylabel(\"Neurons\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot heatmaps for z-scored data: CA3 Sessions\n",
    "\n",
    "pivoted_binned_zscore_neurons = CA3_z_scored.pivot_table(\n",
    "    index=[\"sessFile\", \"neuron\"], \n",
    "    columns=\"bin_midpoint\", \n",
    "    values=\"z_score\"\n",
    "    ).fillna(0)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "heatmap = sns.heatmap(pivoted_binned_zscore_neurons, cmap=\"viridis\", annot=False, fmt=\".2f\", yticklabels=False)\n",
    "\n",
    "x_labels = heatmap.get_xticklabels()\n",
    "\n",
    "rounded_labels = [f\"{float(label.get_text()):.2f}\" if label.get_text() != \"\" else \"\" for label in x_labels]\n",
    "\n",
    "heatmap.set_xticklabels(rounded_labels, rotation=45, ha=\"right\")\n",
    "\n",
    "\n",
    "plt.title(f\"Z-scored Spike Activity by Boundary Distance: CA3 Sessions\")\n",
    "plt.xlabel(\"Boundary Distance (bin midpoint)\")\n",
    "plt.ylabel(\"Neurons\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count number of neurons by sessFile and region dataframe\n",
    "\n",
    "def count_neurons(dataframe, spk_prefix=\"spkTable\"):\n",
    "\n",
    "    # Create an empty dictionary to hold the counts for each session.\n",
    "    session_neuron_counts = {}\n",
    "\n",
    "    # Iterate over unique sessions.\n",
    "    for sess in dataframe[\"sessFile\"].unique():\n",
    "        # Select only rows for that session.\n",
    "        session = dataframe[dataframe[\"sessFile\"] == sess]\n",
    "        \n",
    "        # Identify spkTable columns that are not entirely NaN for this session.\n",
    "        spk_cols = [col for col in session.columns \n",
    "                    if col.startswith(spk_prefix) and not sess_df[col].isna().all()\n",
    "        ]\n",
    "        \n",
    "        # Store the count for this session.\n",
    "        session_neuron_counts[sess] = len(spk_cols)\n",
    "\n",
    "    # Sum the counts over all sessions\n",
    "    total_neurons = sum(session_neuron_counts.values())\n",
    "\n",
    "    return session_neuron_counts, total_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pursuit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
