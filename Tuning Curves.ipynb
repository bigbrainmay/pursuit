{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages and functions\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pursuit_functions as pursuit\n",
    "    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm \n",
    "from numba import njit\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data set\n",
    "\n",
    "all_pursuit_tasks = pd.read_parquet(\"ca1_ca3_rsc_pursuit_data.parquet\", engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Normalize points and find circle boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all coordinate values below 99th percentile and normalize points for all regions \n",
    "\n",
    "normalized_sessions = pursuit.tuning.normalize_points(all_pursuit_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the mean center and overall radius of the arena for all normalized data points\n",
    "#you can specify the percentile value to be considered for the overall radius; default is 95th percentile\n",
    "#calculates the individual center point for each session\n",
    "\n",
    "circle_boundaries, radius = pursuit.tuning.fit_circle_bounds(normalized_sessions)\n",
    "print(radius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find circumference points for plotting using the center coordinates and overall radius\n",
    "all_circ_points = pursuit.tuning.circumference(circle_boundaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# Plot the laser coordinates and boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot normalized concatenated laser and rat paths with center point and boundary\n",
    "#the function takes the normalized_sessions, circle_boundaries, and all_circ_points dataframes\n",
    "\n",
    "pursuit.tuning.plot_arena_bounds(normalized_sessions, circle_boundaries, all_circ_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "# Clean data and pull spike data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain region-specific sessions\n",
    "RSC_sessions = all_pursuit_tasks[all_pursuit_tasks[\"region\"] == \"RSC\"]\n",
    "CA1_sessions = all_pursuit_tasks[all_pursuit_tasks[\"region\"] == \"CA1\"]\n",
    "CA3_sessions = all_pursuit_tasks[all_pursuit_tasks[\"region\"] == \"CA3\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain trial block-specific sessions\n",
    "\n",
    "RSC_pursuit = RSC_sessions[RSC_sessions[\"trial_block\"] == \"pursuit\"]\n",
    "CA1_pursuit = CA1_sessions[CA1_sessions[\"trial_block\"] == \"pursuit\"]\n",
    "CA3_pursuit = CA3_sessions[CA3_sessions[\"trial_block\"] == \"pursuit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop NA values for RSC, CA1, and CA3 sessions\n",
    "\n",
    "RSC_cleaned = pursuit.tuning.drop_NA_vals(RSC_pursuit)\n",
    "CA1_cleaned = pursuit.tuning.drop_NA_vals(CA1_pursuit)\n",
    "CA3_cleaned = pursuit.tuning.drop_NA_vals(CA3_pursuit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# Normalizing time for bootstrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there's some data compression that collapses the 120hz recording time points as the recordings get longer. \n",
    "# first we collapse the time points to whole seconds\n",
    "# we need to normalize the time so that the first observation starts from 0 seconds\n",
    "# we then calculate the normalized minute each observation belongs to\n",
    "\n",
    "def normalize_time(dataframe):\n",
    "    df = dataframe.copy()\n",
    "    df[\"time\"] = df[\"time\"].astype(float)\n",
    "    df[\"relative_time\"] = df.groupby(\"sessFile\")[\"time\"].transform(lambda x: x - x.min())\n",
    "    df[\"norm_sec\"] = df[\"relative_time\"].astype(int)\n",
    "    df[\"norm_min\"] = df[\"norm_sec\"] // 60\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "RSC_clean_time = pursuit.tuning.normalize_time(RSC_cleaned)\n",
    "CA1_clean_time = pursuit.tuning.normalize_time(CA1_cleaned)\n",
    "CA3_clean_time = pursuit.tuning.normalize_time(CA3_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "# Assigning epochs to dataframes with normalized time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to split each session into epochs, we separate them by first/second half of the recording or odd/even minutes\n",
    "# for epoch_half, True:1, False:2 will result in the first half labeled as 1 (<= cutoff) and the second half labeled as 2\n",
    "# for epoch_odd_even, we take the minutes divided by 2 and find the remainder. If the remainder is 1 (odd), it will be labeled as 1 and if the remainder is 0 (even), it will be labeled as 2\n",
    "\n",
    "def assign_epochs(dataframe):\n",
    "    df = dataframe.copy()\n",
    "\n",
    "    # separate epochs by half\n",
    "\n",
    "    def label_half(group):\n",
    "        mins = group[\"norm_min\"].unique()\n",
    "        mins.sort()\n",
    "        cutoff = mins[len(mins) // 2]\n",
    "        return group[\"norm_min\"] <= cutoff\n",
    "    \n",
    "    df[\"epoch_half\"] = (\n",
    "        df.groupby(\"sessFile\", group_keys=False)\n",
    "        .apply(label_half, include_groups=False)\n",
    "        .map({True: 1, False: 2})\n",
    "    )\n",
    "\n",
    "    # separate epochs by odd/even minutes\n",
    "\n",
    "    df[\"epoch_odd_even\"] = df[\"norm_min\"] % 2\n",
    "    df[\"epoch_odd_even\"] = df[\"epoch_odd_even\"].map({1: 1, 0: 2})\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch_half, True:1, False:2 will result in the first half labeled as 1 (<= cutoff) and the second half labeled as 2\n",
    "# for epoch_odd_even, we take the minutes divided by 2 and find the remainder. If the remainder is 1 (odd), it will be labeled as 1 and if the remainder is 0 (even), it will be labeled as 2\n",
    "\n",
    "RSC_clean_time_epochs = pursuit.tuning.assign_epochs(RSC_clean_time)\n",
    "CA1_clean_time_epochs = pursuit.tuning.assign_epochs(CA1_clean_time)\n",
    "CA3_clean_time_epochs = pursuit.tuning.assign_epochs(CA3_clean_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "# Create concise dataframes with only sessFile, laser, epoch, spike, and relative time data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a new df with only sessFile, laser, epoch, spike, and relative time data.\n",
    "def epoch_laser_spks(dataframe, laser_x=\"laserPos_1\", laser_y=\"laserPos_2\"):\n",
    "\n",
    "    epoch_laser_spks_data = []\n",
    "\n",
    "    spk_columns = [col for col in dataframe.columns if \"spkTable\" in col]\n",
    "\n",
    "    time_column = [col for col in dataframe.columns if \"relative_time\" in col.lower()]\n",
    "    \n",
    "    for sessFile in dataframe[\"sessFile\"].unique():\n",
    "\n",
    "            session = dataframe[dataframe[\"sessFile\"] == sessFile].copy()\n",
    "            \n",
    "            laser_x_vals = session[laser_x].astype(\"float64\")\n",
    "            laser_y_vals = session[laser_y].astype(\"float64\")\n",
    "    \n",
    "            #identify 99th percentile x, y boundaries\n",
    "            x_low, x_high = np.percentile(laser_x_vals, [0, 99])\n",
    "            y_low, y_high = np.percentile(laser_y_vals, [0, 99])\n",
    "\n",
    "            #filter the data so we only get the data under the 99th percentile\n",
    "            filter = (\n",
    "                (laser_x_vals >= x_low) & (laser_x_vals <= x_high) & \n",
    "                (laser_y_vals >= y_low) & (laser_y_vals <= y_high)\n",
    "            )\n",
    "\n",
    "            filtered_session = session[filter].copy()\n",
    "\n",
    "            #normalize the points to the origin\n",
    "            x_normalized = filtered_session[laser_x].astype(\"float64\") - float(x_low)\n",
    "            y_normalized = filtered_session[laser_y].astype(\"float64\") - float(y_low)\n",
    "\n",
    "            #grab epoch data \n",
    "            epoch_half = filtered_session[\"epoch_half\"].values\n",
    "            epoch_odd_even = filtered_session[\"epoch_odd_even\"].values\n",
    "\n",
    "            #make a dataframe containing normalized data\n",
    "            normalized_df = pd.DataFrame({\n",
    "                \"sessFile\": sessFile,\n",
    "                \"laser_x_normalized\": x_normalized.values,\n",
    "                \"laser_y_normalized\": y_normalized.values,\n",
    "                \"epoch_half\": epoch_half,\n",
    "                \"epoch_odd_even\": epoch_odd_even\n",
    "            })\n",
    "\n",
    "            #grab spike data using the normalized data mask\n",
    "            spk_df = filtered_session[spk_columns].reset_index(drop=True)\n",
    "\n",
    "            #grab time data using the normalized data mask\n",
    "            time_df = filtered_session[time_column].reset_index(drop=True)\n",
    "\n",
    "            #make a combined dataframe\n",
    "            combined_df = pd.concat([normalized_df.reset_index(drop=True), spk_df, time_df], axis=1)\n",
    "\n",
    "            #append dataframe to the list\n",
    "            epoch_laser_spks_data.append(combined_df)\n",
    "\n",
    "        #make a giant dataframe by concatenating all the dataframes in the list        \n",
    "    epoch_laser_spks_df = pd.concat(epoch_laser_spks_data, ignore_index=True)\n",
    "\n",
    "    return epoch_laser_spks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a new df with only sessFile, laser, epoch, spike, and relative time data.\n",
    "\n",
    "RSC_epoch_laser_spks = pursuit.tuning.epoch_laser_spks(RSC_clean_time_epochs)\n",
    "CA1_epoch_laser_spks = pursuit.tuning.epoch_laser_spks(CA1_clean_time_epochs)\n",
    "CA3_epoch_laser_spks = pursuit.tuning.epoch_laser_spks(CA3_clean_time_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "# Pull epoch halves into two data frames for tuning correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for pulling epochs from each session along with associated sessFile, laser, and spike data\n",
    "\n",
    "def pull_epochs(dataframe, \n",
    "                spk_prefix=\"spkTable\"):\n",
    "\n",
    "    epoch_first_half = []\n",
    "    epoch_second_half = []\n",
    "    epoch_odd_min = []\n",
    "    epoch_even_min = []\n",
    "\n",
    "    for sessFile in dataframe[\"sessFile\"].unique():\n",
    "        \n",
    "        session = dataframe[dataframe[\"sessFile\"] == sessFile].copy()\n",
    "\n",
    "        # mapping first half and second epochs for each session \n",
    "        first_half = session[session[\"epoch_half\"] == 1]\n",
    "        second_half = session[session[\"epoch_half\"] ==2]\n",
    "        # mapping odd and even epochs for each session\n",
    "        odd_min = session[session[\"epoch_odd_even\"] == 1]\n",
    "        even_min = session[session[\"epoch_odd_even\"] == 2]\n",
    "\n",
    "        spk_cols = [col for col in session.columns if spk_prefix in col and not session[col].isna().all()]\n",
    "\n",
    "        #function for grabbing sessFile, laser x, laser y, and spk columns for each epoch\n",
    "        def build_epoch_df(epoch):\n",
    "            return pd.concat([\n",
    "                epoch[[\"sessFile\", \"laser_x_normalized\", \"laser_y_normalized\", \"relative_time\"]].reset_index(drop=True),\n",
    "                epoch[spk_cols].reset_index(drop=True)\n",
    "            ], axis=1)\n",
    "\n",
    "        epoch_first_half.append(build_epoch_df(first_half))\n",
    "        epoch_second_half.append(build_epoch_df(second_half))\n",
    "        epoch_odd_min.append(build_epoch_df(odd_min))\n",
    "        epoch_even_min.append(build_epoch_df(even_min))\n",
    "\n",
    "    return (\n",
    "        pd.concat(epoch_first_half, ignore_index=True),\n",
    "        pd.concat(epoch_second_half, ignore_index=True),\n",
    "        pd.concat(epoch_odd_min, ignore_index=True),\n",
    "        pd.concat(epoch_even_min, ignore_index=True),\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "RSC_epoch_first_half, RSC_epoch_second_half, RSC_epoch_odd_min, RSC_epoch_even_min = pursuit.tuning.pull_epochs(RSC_epoch_laser_spks)\n",
    "CA1_epoch_first_half, CA1_epoch_second_half, CA1_epoch_odd_min, CA1_epoch_even_min = pursuit.tuning.pull_epochs(CA1_epoch_laser_spks)\n",
    "CA3_epoch_first_half, CA3_epoch_second_half, CA3_epoch_odd_min, CA3_epoch_even_min = pursuit.tuning.pull_epochs(CA3_epoch_laser_spks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "# Bootstrapping Functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boostrapping function calculates the true corr then uses the relative time to shift the spktables before calculating the distance, bin spks/laser vals, and calculating tuning\n",
    "\n",
    "@njit\n",
    "def make_assignment_matrix(bin_assignments, dist_bin_edges):\n",
    "    bin_ids = np.digitize(bin_assignments, dist_bin_edges, right=False) - 1\n",
    "    bin_mask = (bin_ids >= 0) & (bin_ids < len(dist_bin_edges) -1)\n",
    "\n",
    "    B = len(dist_bin_edges) - 1\n",
    "    T = len(bin_assignments)\n",
    "    M = np.zeros((B, T))\n",
    "    for t in range(T):\n",
    "        if bin_mask[t]:\n",
    "            M[bin_ids[t], t] += 1\n",
    "    return M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def tuning_calc(M, spk_array):\n",
    "    spk_sum = M @ spk_array # (B, N)\n",
    "    occupancy = M.sum(axis=1) # (B,)\n",
    "\n",
    "    tuning = np.zeros_like(spk_sum)\n",
    "\n",
    "    for b in range(spk_sum.shape[0]):\n",
    "            if occupancy[b] != 0:\n",
    "                for n in range(spk_sum.shape[1]):\n",
    "                    tuning[b, n] = spk_sum[b, n] / occupancy[b]    \n",
    "    \n",
    "    return tuning, occupancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_shift_idx(rel_time, num_shifts=10):\n",
    "    rel_time_np = rel_time.to_numpy().flatten()\n",
    "    shift_points = np.where(np.diff(np.floor(rel_time_np)) > 0)[0] + 1\n",
    "    return shift_points[:num_shifts].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_session(epoch_df1, epoch_df2, center_df, sessFile, spk_cols, num_shifts, rel_time_col=\"relative_time\"):\n",
    "        \n",
    "    all_results = []\n",
    "\n",
    "    valid_spk_cols = [\n",
    "        col for col in spk_cols\n",
    "        if not (epoch_df1[col].isna().all() or epoch_df1[col].sum() == 0) and\n",
    "            not (epoch_df2[col].isna().all() or epoch_df2[col].sum() == 0)\n",
    "        ]\n",
    "\n",
    "    # calculate distance to bounds\n",
    "    dist1 = pursuit.tuning.dist_to_bounds(epoch_df1, center_df)[\"bound_dist\"].values\n",
    "    dist2 = pursuit.tuning.dist_to_bounds(epoch_df2, center_df)[\"bound_dist\"].values\n",
    "\n",
    "    # get spike arrays\n",
    "    spk1 = epoch_df1[valid_spk_cols].to_numpy(dtype=np.float64)\n",
    "    spk2 = epoch_df2[valid_spk_cols].to_numpy(dtype=np.float64)\n",
    "\n",
    "    # get bin edges\n",
    "    dist_bin_edges = np.linspace(min(dist1.min(), dist2.min()),\n",
    "                                max(dist1.max(), dist2.max()), 21)                                   \n",
    "\n",
    "    # make bin assignment matrices \n",
    "    M1 = make_assignment_matrix(dist1, dist_bin_edges)\n",
    "    M2 = make_assignment_matrix(dist2, dist_bin_edges)\n",
    "\n",
    "    # calculate tuning\n",
    "    tune1, _ = tuning_calc(M1, spk1)\n",
    "    tune2, _ = tuning_calc(M2, spk2)\n",
    "\n",
    "    # spearman corr- iter: 0 (True corr)\n",
    "    for j, ncol in enumerate(valid_spk_cols):\n",
    "        r, p = stats.spearmanr(tune1[:, j], tune2[:, j])\n",
    "        all_results.append({\n",
    "            \"sessFile\": sessFile,\n",
    "            \"neuron\": ncol,\n",
    "            \"bootstrap_iter\": 0,\n",
    "            \"spearman_r\": float(r),\n",
    "            \"p_val\": float(p)\n",
    "        })\n",
    "\n",
    "    #bootstrapping with shift indices from rel_time\n",
    "\n",
    "    rel_time = epoch_df1[rel_time_col]\n",
    "    shift_points = make_shift_idx(rel_time, num_shifts=num_shifts)\n",
    "\n",
    "    for i, shift_idx in enumerate(shift_points, start=1):\n",
    "        spk1_shifted = np.roll(spk1, shift_idx, axis=0) \n",
    "        tune1_rolled, _ = tuning_calc(M1, spk1_shifted)\n",
    "\n",
    "        for j, ncol in enumerate(valid_spk_cols):\n",
    "            r, p = stats.spearmanr(tune1_rolled[:, j], tune2[:, j])\n",
    "            all_results.append({\n",
    "                \"sessFile\": sessFile,\n",
    "                \"neuron\": ncol,\n",
    "                \"bootstrap_iter\": i,\n",
    "                \"spearman_r\": float(r),\n",
    "                \"p_val\": float(p)\n",
    "                })\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_all_sessions(epoch_df1, epoch_df2, center_df, spk_prefix=\"spkTable\", rel_time_col=\"relative_time\", num_shifts=1000):\n",
    "    spk_cols = [col for col in epoch_df1.columns if spk_prefix in col]\n",
    "    sessions = epoch_df1[\"sessFile\"].unique()\n",
    "\n",
    "    all_bootstrap_results = []\n",
    "\n",
    "    for sessFile in tqdm(sessions, desc=\"Sessions\"):\n",
    "        df1 = epoch_df1[epoch_df1[\"sessFile\"] == sessFile]\n",
    "        df2 = epoch_df2[epoch_df2[\"sessFile\"] == sessFile]\n",
    "\n",
    "        results = process_session(df1, df2, center_df, sessFile=sessFile, spk_cols=spk_cols, num_shifts=num_shifts, rel_time_col=rel_time_col)\n",
    "        all_bootstrap_results.extend(results)\n",
    "\n",
    "    return pd.DataFrame(all_bootstrap_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bootstrap sessions\n",
    "RSC_first_second_tuning = pursuit.tuning.bootstrap_all_sessions(RSC_epoch_first_half, RSC_epoch_second_half, circle_boundaries)\n",
    "RSC_odd_even_tuning = pursuit.tuning.bootstrap_all_sessions(RSC_epoch_odd_min, RSC_epoch_even_min, circle_boundaries)\n",
    "\n",
    "CA1_first_second_tuning = pursuit.tuning.bootstrap_all_sessions(CA1_epoch_first_half, CA1_epoch_second_half, circle_boundaries)\n",
    "CA1_odd_even_tuning = pursuit.tuning.bootstrap_all_sessions(CA1_epoch_odd_min, CA1_epoch_even_min, circle_boundaries)\n",
    "\n",
    "CA3_first_second_tuning = pursuit.tuning.bootstrap_all_sessions(CA3_epoch_first_half, CA3_epoch_second_half, circle_boundaries)\n",
    "CA3_odd_even_tuning = pursuit.tuning.bootstrap_all_sessions(CA3_epoch_odd_min, CA3_epoch_even_min, circle_boundaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "RSC_first_second_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "RSC_odd_even_tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "# Find cells with corr values over the 95th and 99th percentiles and compare them to the true tuning values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_null_dist(df, neurons=None, percentile_lines=(95, 99), max_neurons=None):\n",
    "    grouped = df.groupby([\"sessFile\", \"neuron\"])\n",
    "\n",
    "    if neurons is None:\n",
    "        neurons = list(grouped.groups.keys())\n",
    "        if max_neurons is not None:\n",
    "            neurons = neurons[:max_neurons]\n",
    "\n",
    "    for sessFile, neuron in neurons:\n",
    "        sub_df = grouped.get_group((sessFile, neuron))\n",
    "\n",
    "        null_df = sub_df[sub_df[\"bootstrap_iter\"] > 0]\n",
    "        true_df = sub_df[sub_df[\"bootstrap_iter\"] == 0]\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(8,5))\n",
    "\n",
    "        sns.rugplot(\n",
    "            x=null_df[\"spearman_r\"],\n",
    "            height=0.05,\n",
    "            hue=null_df[\"bootstrap_iter\"],\n",
    "            ax=ax,\n",
    "            lw=0.05\n",
    "        )\n",
    "\n",
    "        sns.histplot(\n",
    "            null_df[\"spearman_r\"],\n",
    "            bins = 30,\n",
    "            stat=\"count\",\n",
    "            kde=False,\n",
    "            color='lightblue',\n",
    "            edgecolor='white',\n",
    "            ax=ax,\n",
    "            alpha=0.6\n",
    "        )\n",
    "\n",
    "        true_r = true_df[\"spearman_r\"].values[0]\n",
    "        ax.axvline(true_r, color='red', linestyle='--', label=f\"True r ={true_r:.3f}\")\n",
    "\n",
    "        for p in percentile_lines:\n",
    "            threshold = np.percentile(null_df[\"spearman_r\"], p)\n",
    "            ax.axvline(threshold, color='gray', linestyle=':', label=f\"{p}th ={threshold:.3f}\")\n",
    "\n",
    "        ax.set_title(f\"Null dist — {sessFile} / {neuron}\")\n",
    "        ax.set_xlabel(\"Spearman r\")\n",
    "        ax.set_ylabel(\"Bootstrap count\")\n",
    "        ax.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot null distribution of spearman r values with true, 95th, 99th percentile lines for all cells\n",
    "pursuit.tuning.plot_null_dist(RSC_first_second_tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're going to get the significant cells now\n",
    "\n",
    "def get_significant_cells(df, percentile=95):\n",
    "    significant_cells = []\n",
    "\n",
    "    grouped = df.groupby([\"sessFile\", \"neuron\"])\n",
    "\n",
    "    for (sessFile, neuron), group in grouped:\n",
    "        true_r = group.loc[group[\"bootstrap_iter\"] == 0, \"spearman_r\"].values\n",
    "        true_r = true_r[0]\n",
    "\n",
    "        boot_r = group.loc[group[\"bootstrap_iter\"] > 0, \"spearman_r\"]\n",
    "        threshold = np.percentile(boot_r, percentile)\n",
    "\n",
    "        if true_r > threshold:\n",
    "            significant_cells.append((sessFile, neuron))\n",
    "\n",
    "    return significant_cells    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ID significant cells above 95th and 99th corr values\n",
    "RSC_first_second_cells_95 = pursuit.tuning.get_significant_cells(RSC_first_second_tuning, percentile=95)\n",
    "RSC_first_second_cells_99 = pursuit.tuning.get_significant_cells(RSC_first_second_tuning, percentile=99)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot null distribution of spearman r values with true, 95th, 99th percentile lines for significant cells\n",
    "pursuit.tuning.plot_null_dist(RSC_first_second_tuning, neurons=RSC_first_second_cells_95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig_cells_heatmap(raw_data, center_df, sig_cells_list, percentile=95,\n",
    "                                 smoothing_window=3, smoothing_std=1, plot_title=None):\n",
    "    \n",
    "    df_sig_cells = pd.DataFrame(sig_cells_list, columns=[\"sessFile\", \"neuron\"])\n",
    "\n",
    "   \n",
    "    df_laser_spks = pursuit.tuning.norm_laser_get_spks(raw_data)\n",
    "\n",
    "   \n",
    "    df_bounds = pursuit.tuning.dist_to_bounds(df_laser_spks, center_df)\n",
    "\n",
    "    \n",
    "    df_binned = pursuit.tuning.bin_spikes_laser(df_bounds)\n",
    "\n",
    "    \n",
    "    df_tuning = pursuit.tuning.calculate_tuning(df_binned)\n",
    "\n",
    "   \n",
    "    df_tuning_filtered = df_tuning.merge(df_sig_cells, on=[\"sessFile\", \"neuron\"])\n",
    "\n",
    "   \n",
    "    df_z = pursuit.tuning.z_score_norm(df_tuning_filtered)\n",
    "\n",
    "   \n",
    "    df_smoothed = pursuit.tuning.pivot_smooth(df_z, window_size=smoothing_window, std=smoothing_std)\n",
    "\n",
    "   \n",
    "    df_sorted = pursuit.tuning.peak_sort(df_smoothed)\n",
    "\n",
    "   \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(df_sorted, cmap=\"viridis\", annot=False, fmt=\".2f\", yticklabels=False)\n",
    "    plt.title(plot_title or f\"Z-scored Spike Activity (> {percentile}th percentile)\")\n",
    "    plt.xlabel(\"Boundary Distance (bin midpoint)\")\n",
    "    plt.ylabel(\"Neurons (peak sorted)\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return df_sorted, df_binned  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot heatmaps of significant cells and return two dataframes; one for peak-sorted neurons ready to plot and one with binned tuning values \n",
    "df_sorted , df_binned = pursuit.tuning.sig_cells_heatmap(RSC_cleaned, circle_boundaries, RSC_first_second_cells_95, percentile=95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "def occ_dist_to_pdf(df, output_path = \"RSC_sessions_laser_occ.pdf\"):\n",
    "    \n",
    "    grouped = df.groupby([\"sessFile\", \"bin_midpoint\"], observed=True)[\"laser_occupancy\"].first().reset_index()\n",
    "\n",
    "    occ_stats = (\n",
    "        grouped.groupby(\"bin_midpoint\", observed=True)[\"laser_occupancy\"]\n",
    "        .agg([\"mean\"])\n",
    "        .reset_index()\n",
    "        .rename(columns={\"mean\": \"mean_occ\"})\n",
    "    )\n",
    "\n",
    "    merged = grouped.merge(occ_stats, on=\"bin_midpoint\", how=\"left\")\n",
    "\n",
    "    with PdfPages(output_path) as pdf:\n",
    "\n",
    "        for sessFile, group in merged.groupby(\"sessFile\"):\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "            sns.barplot(\n",
    "                x=group[\"bin_midpoint\"].astype(str),\n",
    "                y=group[\"laser_occupancy\"],\n",
    "                ax=ax,\n",
    "                color='lightblue',\n",
    "                edgecolor='black',\n",
    "                label=\"Session Laser Occupancy\"\n",
    "            )\n",
    "\n",
    "            ax.errorbar(\n",
    "                x=np.arange(len(group)),\n",
    "                y=group[\"mean_occ\"],\n",
    "                fmt='o',\n",
    "                color='black',\n",
    "                capsize=4,\n",
    "                label=\"Global Mean\"\n",
    "            )\n",
    "\n",
    "            ax.set_title(f\"Laser Occupancy by Bin - {sessFile}\")\n",
    "            ax.set_xlabel(\"Boundary Distance (bin midpoint)\")\n",
    "            ax.set_ylabel(\"Laser Occupancy\")\n",
    "            plt.xticks(rotation=45, ha=\"right\")\n",
    "            ax.legend()\n",
    "            plt.tight_layout()\n",
    "            #plt.show()\n",
    "            \n",
    "            pdf.savefig(fig)\n",
    "            plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot at total laser occupancy per bin per session (and save to pdf bc there's a lot of plots)\n",
    "\n",
    "pursuit.tuning.occ_dist_to_pdf(df_binned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sig_tuning_curves(raw_data, center_df, sig_cells_list, percentile=95,\n",
    "                                 smoothing_window=3, smoothing_std=1, plot_title=None):\n",
    "    \n",
    "    df_sig_cells = pd.DataFrame(sig_cells_list, columns=[\"sessFile\", \"neuron\"])\n",
    "\n",
    "   \n",
    "    df_laser_spks = pursuit.tuning.norm_laser_get_spks(raw_data)\n",
    "\n",
    "   \n",
    "    df_bounds = pursuit.tuning.dist_to_bounds(df_laser_spks, center_df)\n",
    "\n",
    "    \n",
    "    df_binned = pursuit.tuning.bin_spikes_laser(df_bounds)\n",
    "\n",
    "    \n",
    "    df_tuning = pursuit.tuning.calculate_tuning(df_binned)\n",
    "\n",
    "    \n",
    "    df_tuning_filtered = df_tuning.merge(df_sig_cells, on=[\"sessFile\", \"neuron\"])\n",
    "\n",
    "\n",
    "    grouped = df_tuning_filtered.groupby([\"sessFile\", \"neuron\"])\n",
    "\n",
    "    for (sessFile, neuron), sub_df in grouped:\n",
    "        \n",
    "        pivoted = (sub_df.pivot(index=\"neuron\", columns=\"bin_midpoint\", values=\"tuning\").fillna(0))\n",
    "\n",
    "        for neuron in pivoted.index:\n",
    "            plt.plot(pivoted.columns, pivoted.loc[neuron], marker='o', linestyle='-', label=f\"{neuron}\")\n",
    "\n",
    "        plt.xlabel(\"Boundary Distance (bin midpoint)\")\n",
    "        plt.ylabel(\"Tuning (spike count / laser occupancy)\")\n",
    "        plt.title(f\"Tuning Curve — {sessFile} / {neuron}\")\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot individual significant cell tuning curves\n",
    "\n",
    "pursuit.tuning.plot_sig_tuning_curves(RSC_cleaned, circle_boundaries, RSC_first_second_cells_95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "# ignore everything under here for now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "# Investigating time shifts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_time_entries(dataframe):\n",
    "    all_sessions = []\n",
    "\n",
    "    for sessFile in dataframe[\"sessFile\"].unique():\n",
    "        session = dataframe[dataframe[\"sessFile\"] == sessFile]\n",
    "        session_times = session[\"time\"]\n",
    "\n",
    "        times_unique = session_times.astype(\"float64\").nunique()\n",
    "        times_count = session_times.astype(\"float64\").value_counts().to_dict()\n",
    "\n",
    "        all_sessions.append({\n",
    "            \"sessFile\": sessFile,\n",
    "            \"times_unique\": times_unique,\n",
    "            \"unique_times_count\": times_count\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(all_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "RSC_sessions_times = unique_time_entries(RSC_sessions)\n",
    "RSC_sessions_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_time_counts(session_row):\n",
    "    times= session_row[\"unique_times_count\"]\n",
    "    sorted_times = sorted(times.items())\n",
    "    return pd.DataFrame(sorted_times, columns=[\"time\", \"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "lp03_25 = RSC_sessions_times[RSC_sessions_times[\"sessFile\"] == \"LP03_25_pursuitRoot.mat\"]\n",
    "\n",
    "lp03_25_row = lp03_25.iloc[0]\n",
    "\n",
    "lp03_25_inspection = inspect_time_counts(lp03_25_row)\n",
    "\n",
    "\n",
    "lp03_25_inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "# Find distance of laser points to boundary and bin data by distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find distance of normalized laser points to circle boundary by each session\n",
    "#function takes the normalized laser/spikes and circle boundaries dataframes\n",
    "\n",
    "RSC_laser_spks_bounds = pursuit.tuning.dist_to_bounds(RSC_laser_spks, circle_boundaries)\n",
    "CA1_laser_spks_bounds = pursuit.tuning.dist_to_bounds(CA1_laser_spks, circle_boundaries)\n",
    "CA3_laser_spks_bounds = pursuit.tuning.dist_to_bounds(CA3_laser_spks, circle_boundaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "CA3_laser_spks_bounds.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bin the data!\n",
    "RSC_laser_spikes_binned = pursuit.tuning.bin_spikes_laser(RSC_laser_spks_bounds)\n",
    "CA1_laser_spikes_binned = pursuit.tuning.bin_spikes_laser(CA1_laser_spks_bounds)\n",
    "CA3_laser_spikes_binned = pursuit.tuning.bin_spikes_laser(CA3_laser_spks_bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "# Normalize spike counts, smooth data, peak sort neurons, and plot tuning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate raw tuning curves\n",
    "RSC_tuning = pursuit.tuning.calculate_tuning(RSC_laser_spikes_binned)\n",
    "CA1_tuning = pursuit.tuning.calculate_tuning(CA1_laser_spikes_binned)\n",
    "CA3_tuning = pursuit.tuning.calculate_tuning(CA3_laser_spikes_binned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot all neuron tuning curves for a sanity check\n",
    "\n",
    "pursuit.tuning.plot_tuning_curves(CA3_tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# z-score and normalize data\n",
    "RSC_z_scored = pursuit.tuning.z_score_norm(RSC_tuning)\n",
    "CA1_z_scored = pursuit.tuning.z_score_norm(CA1_tuning)\n",
    "CA3_z_scored = pursuit.tuning.z_score_norm(CA3_tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply smoothing and pivot data\n",
    "RSC_smoothed = pursuit.tuning.pivot_smooth(RSC_z_scored)\n",
    "CA1_smoothed = pursuit.tuning.pivot_smooth(CA1_z_scored)\n",
    "CA3_smoothed = pursuit.tuning.pivot_smooth(CA3_z_scored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# peak sort the data\n",
    "RSC_smoothed_sorted = pursuit.tuning.peak_sort(RSC_smoothed)\n",
    "CA1_smoothed_sorted = pursuit.tuning.peak_sort(CA1_smoothed)\n",
    "CA3_smoothed_sorted = pursuit.tuning.peak_sort(CA3_smoothed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot heatmaps\n",
    "pursuit.tuning.heatmap(RSC_smoothed_sorted)\n",
    "pursuit.tuning.heatmap(CA1_smoothed_sorted)\n",
    "pursuit.tuning.heatmap(CA3_smoothed_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "# Calculate neuron counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "RSC_count = pursuit.tuning.count_neurons(RSC_cleaned)\n",
    "CA1_count = pursuit.tuning.count_neurons(CA1_cleaned)\n",
    "CA3_count = pursuit.tuning.count_neurons(CA3_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "RSC_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "CA1_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "CA3_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pursuit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
